URL Click Analysis
========================================================

This is an R Markdown document. Markdown is a simple formatting syntax for authoring web pages where you can embed R codes. 

The [source data](https://s3.amazonaws.com/coursera_intro_data_science_project/coursera.sanitized.csv). Special thanks to Nandita for coming up with an idea for processing URL and Refer URL variables.


Data inspection and cleanup
---------------------------

### load raw data
```{r}
# raw <- read.csv("coursera.sanitized.csv")
# sapply(raw[1, ], class)
```

### click: 
An integer representing whether or not a click occurred - this is the outcome we want to predict
```{r}
# table(raw$click)
# just those actually got any clicks
# clicked <- raw[raw$click==1,]
```

### datehour: 
A string describing the day and hour when the ad was served
```{r}
# head(raw$datehour)
# sum(is.na(raw$datehour))
# convert string to datetime object
# pdata <- raw
# datehour <- strptime(raw$datehour, "%Y-%m-%d %H:%M:%S")
# pdata$wday <- datehour$wday
# pdata$hour <- datehour$hour
# pdata$datehour <- datehour
```

### v_id: 
An integer representing a channel in which a viewer has been served through. 
```{r}
# table(pdata$v_id)
# just those actually got any clicks
# table(clicked$v_id)
# check missing values
# sum(is.na(pdata$v_id))
```
### b_id: 
An integer representing a class of user based on their interests
```{r}
# length(unique(pdata$b_id))
# just those actually got any clicks
# length(unique(clicked$b_id))
# check missing values
# sum(is.na(pdata$b_id))
# inspect data
# head(pdata$b_id)
# check the number of NULL values
# sum(pdata$b_id=='\\N')
# rename "\\N" to 0 and convert the factor into integer
# levels(pdata$b_id)[levels(pdata$b_id) == '\\N'] <- "0"
# pdata$b_id <- as.numeric(as.character(pdata$b_id))
```
### t_id: 
An integer identifying a specific ad
```{r}
# length(unique(pdata$t_id))
# just those actually got any clicks
# length(unique(clicked$t_id))
# check missing values
# sum(is.na(pdata$t_id))
```
### seller: 
An integer representing the seller providing the wholesaler with the impression
```{r}
# length(unique(pdata$seller))
# just those actually got any clicks
# length(unique(clicked$seller))
# check missing values
# sum(is.na(pdata$seller))
# check the value range
# min(pdata$seller[!is.na(pdata$seller)])
# max(pdata$seller[!is.na(pdata$seller)])
# pdata$seller[is.na(pdata$seller)] <- 3000
# sum(is.na(pdata$seller))
```
### country: 
A string representing the clicker's country of origin
```{r}
# table(pdata$country)
# just those actually got any clicks
# table(clicked$country)
# check NULL values
# levels(pdata$country)
# levels(pdata$country)[levels(pdata$country) == ""] <- "00"
# levels(pdata$country)
```
### state: 
A string representing the clicker's state of origin
```{r}
# length(unique(pdata$state))
# just those actually got any clicks
# length(unique(clicked$state))
# check NULL values
# levels(pdata$state)
# levels(pdata$state)[levels(pdata$state) == ""] <- "00"
# levels(pdata$state)
```
### url: 
An encrypted string representing the URL the ad was displayed on
```{r}
# length(unique(pdata$url))
# just those actually got any clicks
# length(unique(clicked$url))
# check NULL values
# levels(pdata$url)[levels(pdata$url) == ""]
```

Check the frequency of urls = views. Special thanks to Nandita for coming up with this metric. 
```{r fig.width=7, fig.height=6}
# quantile(table(pdata$url))
# barplot(sort(table(pdata$url),decreasing=TRUE), col="blue")
# barplot(sort(table(clicked$url),decreasing=TRUE), col="blue")
# views <- as.data.frame(table(pdata$url))
# names(views) <- c("url", "views")
```

Check the number of traffic sources - this represents the popularity of a given URL similar to PageRank algorithm. This is also based on Nandita's idea.
```{r}
sourceCount <- function(page_url) {
  ref <- pdata$refer_url[pdata$url==page_url]
  return(length(unique(ref)))
}

# exetime <- system.time(views$srcs <- sapply(views[,1], sourceCount))
# exetime[3]/60
```

Add the views and source counts to the data
```{r}
# pdata <- merge(pdata,views,all=TRUE)
```

### refer_url: 
An encrypted string representing the referrer URL
```{r}
# length(unique(pdata$refer_url))
# just those actually got any clicks
# length(unique(clicked$refer_url))
# check NULL values
# levels(pdata$refer_url)[levels(pdata$refer_url) == ""]
```

Check the frequency of refer_urls
```{r fig.width=7, fig.height=6}
# quantile(table(pdata$refer_url))
# barplot(sort(table(pdata$refer_url),decreasing=TRUE), col="blue")
# barplot(sort(table(clicked$refer_url),decreasing=TRUE), col="blue")
```

Add the frequency to the data - this represents how frequently a given referrer sends traffic - the larger frequency, the more active. 
```{r}
# referrals <- as.data.frame(table(pdata$refer_url))
# names(referrals) <- c("refer_url", "referrals")
# pdata <- merge(pdata,referrals,all=TRUE)
```

### Check the cleanup result
```{r}
# pdata <- pdata[,c("datehour","wday","hour", "v_id", "b_id", "t_id", "seller", "country", "state", "views", "srcs", "referrals", "url", "refer_url", "click")]
# summary(pdata)
# head(pdata)
# save(pdata, file="processed.rda")
load(file="processed.rda")
```

Deal with the class imbalance
---------------------------------
### Reduce the majority class by subsumpling

Here we are making an assumption that we can ignore most of the cases where a click didn't occur without impeding our ability to detect the cases where it did occur.

```{r}
# first check the ratio between the two classes
table(pdata$click)

# drop url and refer_url variables
pdata <- pdata[,!(names(pdata) %in% c("datehour","url","refer_url"))]
# convert click to a factor
pdata$click <- as.factor(pdata$click)

# subample the super majority class to reduce the ratio to 10:1
maj <- pdata[pdata$click==0,]
subSampling <- sample(1:dim(maj)[1], size=217*10, replace=FALSE)
subSampled <- rbind(pdata[pdata$click==1,],maj[subSampling,])
table(subSampled$click)
summary(subSampled)
```

### Setup Cross Validation

In order to test the predictive model, we will split the downsampled data into two subsets. We will also take a sampling from the original data without downsampling the majority class.

```{r}
# split data into two subsets - 2/3 training set, 1/3 test set
set.seed(333)
trainSamples <- sample(1:dim(subSampled)[1], size=(dim(subSampled)[1]/3*2), replace=FALSE)
train <- subSampled[trainSamples,]
test <- subSampled[-trainSamples,]
# we will also have a validation set from the original data
valSamples <- sample(1:dim(pdata)[1], size=dim(pdata)[1]/50, replace=FALSE)
validation <- pdata[valSamples,]
# check the number of minority class - should be around 145:72
sum(train$click==1)
sum(test$click==1)
sum(validation$click==1)
```

Random Forest
--------------

We try to build a predictive model using a random forest, with parameters set to take 10 samples from each class per iteration to make sure we get a balanced result. 

```{r}
suppressPackageStartupMessages(library(randomForest))
table(train$click)
set.seed(1234)
rf.model1 <- randomForest(click ~., data=train, importance=TRUE, prox=TRUE, strata=train$click, sampsize=c(10,10))
rf.model1
```

The class error rate for click = 1 is 35.0% - which is not so great, but at least we are getting more than half of them right. 

Now let's see how much accuracy we get on the test set. 

```{r}
# make prediction from the test set
test.pred1 <- predict(rf.model1,test[,-12])
# compare it to the actual outcome
confusionMatrix <- table(observed=test$click,predicted=test.pred1)
confusionMatrix
# class error rate of click=1
confusionMatrix[2,1]/sum(confusionMatrix[2,])
```

The class error rate is 29.9% - a bit better.

Now let's make prediction with validation set

```{r}
# make prediction from the validation set
validation.pred1 <- predict(rf.model1,validation[,-12])
# compare it to the actual outcome
confusionMatrix <- table(observed=validation$click,predicted=validation.pred1)
confusionMatrix
# class error rate of click=1
confusionMatrix[2,1]/sum(confusionMatrix[2,])
```

The class error rate is 25.0% - we called 3 out of 4 correctly.
 
```{r}
result <-importance(rf.model1,)[,"MeanDecreaseAccuracy"]
importance(rf.model1)[order(result,decreasing=TRUE),]
```

The result rank the variable's importance by how much each contribute to reduce prediction error. However, the differences are pretty small, so I am not confident that this is a robust result.

Improving the model
-------------------

Wenjia points out that two variables that show high importance, seller and b_id, happen to contain a lot of NULL values. These NULL values may be artificially raising the apparent importance of those variables.

### Seller variable

Let's examine "seller" variable.
```{r}
# 94129 null values in "seller" variable were re-coded with 3000
table(pdata$seller[pdata$seller==3000], pdata$v_id[pdata$seller==3000])
length(pdata$v_id[pdata$v_id==3])
```

It looks none of the v_id==3 conains valid seller id. 
So what happens if we remove v_id==3?

```{r}
# New random forest without v_id==3
rf.model2 <- randomForest(click ~., data=train[train$v_id!=3,], importance=TRUE, prox=TRUE, strata=train$click[train$v_id!=3], sampsize=c(10,10))
rf.model2
result <-importance(rf.model2,)[,"MeanDecreaseAccuracy"]
importance(rf.model2)[order(result,decreasing=TRUE),]
```

The importance of "seller" variable drops significantly once the null values are removed. For this reason, we can probably ignore this variable altogether. 

```{r}
# New random forest without seller variable
noseller <- train
noseller$seller <- NULL
rf.model3 <- randomForest(click ~., data=noseller, importance=TRUE, prox=TRUE, strata=noseller$click, sampsize=c(10,10))
rf.model3
result <-importance(rf.model3,)[,"MeanDecreaseAccuracy"]
importance(rf.model3)[order(result,decreasing=TRUE),]
```

### b_id variable

Let's now examine "b_id" variable. 546712 values "//N" variable were re-coded with 0.

```{r}
# how many clicks does that class contain?
table(pdata$click[pdata$b_id==0])
# how many clicks do all other classes contain?
table(pdata$click[pdata$b_id!=0])
```

It turned out Null value is a majority class for this variable. So what happens if we remove it?

```{r}
# New random forest without null values in b_id
nonulls <- noseller[noseller$b_id!=0,]
rf.model4 <- randomForest(click ~., data=nonulls, importance=TRUE, prox=TRUE, strata=nonulls$click, sampsize=c(10,10))
rf.model4
result <-importance(rf.model4,)[,"MeanDecreaseAccuracy"]
importance(rf.model4)[order(result,decreasing=TRUE),]
```

b_id still remains high, and our class error rate worsened noticeably. So it is probably not good idea to remove the records with null values. We should keep those records, but we shouldn't use this variable because of the class imbalance. 

```{r}
# New random forest without b_id
noBID <- noseller
noBID$b_id <- NULL
rf.model5 <- randomForest(click ~., data=noBID, importance=TRUE, prox=TRUE, strata=noBID$click, sampsize=c(10,10))
rf.model5
result <-importance(rf.model5,)[,"MeanDecreaseAccuracy"]
importance(rf.model5)[order(result,decreasing=TRUE),]
```

The important variables are now: views, t_id, srcs, referrals, and state. This seems to make more intuitive sense.

Now let's see how much accuracy we get on the test set. 

```{r}
# drop seller, b_id from the test set
test <- test[, !(names(test) %in% c("seller", "b_id"))]
# make prediction from the test set
test.pred5 <- predict(rf.model5,test[,-10])
# compare it to the actual outcome
confusionMatrix <- table(observed=test$click,predicted=test.pred5)
confusionMatrix
# class error rate of click=1
confusionMatrix[2,1]/sum(confusionMatrix[2,])
```

Now let's make prediction with validation set

```{r}
# drop seller, b_id from the validation set
validation <- validation[, !(names(validation) %in% c("seller", "b_id"))]
# make prediction from the validation set
validation.pred5 <- predict(rf.model5,validation[,-10])
# compare it to the actual outcome
confusionMatrix <- table(observed=validation$click,predicted=validation.pred5)
confusionMatrix
# class error rate of click=1
confusionMatrix[2,1]/sum(confusionMatrix[2,])
```

So we didn't lose our predictive power, either. 